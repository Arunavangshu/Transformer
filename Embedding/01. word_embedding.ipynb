{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06919872-2255-4603-a5d2-85f3c7146382",
   "metadata": {},
   "source": [
    "# Understanding Word Embeddings: The Key to Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5467d-7bd8-4522-802d-330d74bd0f94",
   "metadata": {},
   "source": [
    "### What are Word Embeddings?\n",
    "\n",
    "Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation in a continuous vector space. Instead of representing words as discrete units (e.g., one-hot encoding where each word is represented by a binary vector), word embeddings map words into dense vectors of real numbers. These vectors capture the semantic relationships between words, meaning that words with similar meanings or usage contexts will be closer to each other in this vector space.\n",
    "\n",
    "In simpler terms, word embeddings convert words into numerical representations that the machine can understand while retaining the relationships between them. This enables NLP models to perform better on tasks like text classification, sentiment analysis, machine translation, and more.\n",
    "\n",
    "### Why Are Word Embeddings Important?\n",
    "Before word embeddings, NLP models used simpler, but less efficient methods of representing text, such as one-hot encoding or bag-of-words (BoW). These methods had their limitations:\n",
    "\n",
    "+ **High Dimensionality:** One-hot encoding, for example, creates vectors that are as long as the vocabulary size. For a large corpus, this results in sparse and high-dimensional vectors.\n",
    "+ **Loss of Semantic Meaning:** Traditional methods fail to capture relationships between words. In one-hot encoding, the words \"king\" and \"queen\" would be as unrelated as \"king\" and \"apple,\" even though the former pair is closely related in meaning.\n",
    "+ **Inefficiency in Handling Synonyms:** With one-hot encoding or BoW, synonyms or words with similar meanings would have completely different representations, leading to poor performance in NLP tasks.\n",
    "\n",
    "\n",
    "Word embeddings address these issues by mapping words with similar meanings or usage patterns to nearby points in a vector space, thus facilitating more efficient and accurate language models.\n",
    "\n",
    "\n",
    "### How Do Word Embeddings Work?\n",
    "Word embeddings use machine learning models to learn these dense vector representations. There are several ways to generate word embeddings, but two of the most popular methods are Word2Vec and GloVe.\n",
    "\n",
    "+ **1. Word2Vec (Skip-Gram and CBOW)**\n",
    "\n",
    "Word2Vec is a neural network-based model developed by Google in 2013. It uses a shallow neural network to learn word embeddings by predicting a target word based on its context (the words around it) or vice versa. There are two primary architectures in Word2Vec:\n",
    "\n",
    "**Skip-Gram:** The model tries to predict the context (neighboring words) given a target word.\n",
    "\n",
    "**Continuous Bag of Words (CBOW):** The model predicts the target word based on its context (the surrounding words).\n",
    "Word2Vec works by training on a large corpus of text, where it learns which words tend to appear together in similar contexts. Over time, this results in embeddings that capture semantic relationships, such as \"king\" - \"man\" + \"woman\" = \"queen.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "+ **2. GloVe (Global Vectors for Word Representation)**\n",
    "\n",
    "GloVe, developed by Stanford, is another approach to generating word embeddings. While Word2Vec is a local context model (looking at local windows of words), GloVe is a global model. It creates word embeddings by factoring a word co-occurrence matrix, which records how often words appear together in a corpus. GloVe uses this matrix to capture the global statistical information about the corpus, resulting in embeddings that reflect both local and global word relationships.\n",
    "\n",
    "The key advantage of GloVe over Word2Vec is that it captures a broader, more global perspective on word relationships, leveraging the entire corpus rather than focusing on local context windows.\n",
    "\n",
    "\n",
    "### Word Embedding Applications\n",
    "Word embeddings have found widespread use in a variety of NLP tasks. Some of the most common applications include:\n",
    "\n",
    "**Text Classification:** By converting words into embeddings, classifiers can better understand the meaning of the text, improving the accuracy of tasks like sentiment analysis, spam detection, or topic categorization.\n",
    "\n",
    "**Named Entity Recognition (NER):** Word embeddings help identify entities (like names, locations, and organizations) within text by capturing contextual meaning.\n",
    "\n",
    "**Machine Translation:** Word embeddings enable better translation between languages by encoding semantic meaning, rather than just relying on direct word mapping.\n",
    "\n",
    "**Recommendation Systems:** Embeddings are also used in content-based recommendation systems to suggest items based on semantic similarities between users’ preferences and the items they interact with.\n",
    "\n",
    "**Question Answering and Chatbots:** Embeddings allow models to understand and respond to user queries by capturing the underlying meaning and context of the input.\n",
    "\n",
    "\n",
    "### Challenges of Word Embeddings\n",
    "While word embeddings have revolutionized NLP, they are not without their challenges:\n",
    "\n",
    "**1. Bias:** Word embeddings can encode societal biases present in the training data. For instance, they might reflect gender, racial, or cultural biases, which can lead to problematic outcomes when used in real-world applications.\n",
    "\n",
    "**2. OOV (Out of Vocabulary) Words:** If a word has not been seen during training, it won’t have a corresponding embedding. This can be problematic in handling rare or domain-specific words.\n",
    "\n",
    "**3. Polysemy:** Words that have multiple meanings (e.g., \"bank\" as a financial institution vs. \"bank\" as the side of a river) can be difficult for embeddings to represent accurately, as one embedding cannot fully capture all possible meanings of a word.\n",
    "\n",
    "**4. Dimensionality:** The choice of the dimensionality of the embeddings can impact performance. Too few dimensions may lead to poor performance, while too many dimensions can make the model slow and more prone to overfitting.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Moving Beyond Word-Level Embeddings\n",
    "While word embeddings are incredibly useful, they have some limitations, particularly in dealing with phrases, sentences, or longer contexts. To address these, newer models like contextual embeddings (e.g., BERT, GPT, and ELMo) have been developed. These models create embeddings that dynamically adjust based on the context in which a word appears, providing more accurate representations for words with multiple meanings.\n",
    "\n",
    "For instance, BERT (Bidirectional Encoder Representations from Transformers) generates embeddings not just based on the word itself, but also considering the words before and after it in a sentence. This allows for a more nuanced understanding of meaning and context.\n",
    "\n",
    "### Conclusion\n",
    "Word embeddings have transformed the field of NLP, allowing machines to understand language in a way that is both more efficient and contextually aware. From early methods like Word2Vec and GloVe to more advanced contextual embeddings in models like BERT, word embeddings continue to drive progress in making machines better at understanding human language. Despite challenges like bias and handling polysemy, their importance in applications such as sentiment analysis, machine translation, and recommendation systems cannot be overstated.\n",
    "\n",
    "As NLP technology continues to evolve, so too will the techniques for generating and using word embeddings. For anyone diving into the world of language models, understanding word embeddings is essential to unlocking the power of language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8917e8-a81f-49fc-864e-954a81606d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4942f835-f0c3-470f-b030-aedacfb07df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac544651-fdeb-4419-bca4-3dc2d7aa6fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/arunavangshumaiti/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/arunavangshumaiti/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading NLTK punkt tokenizer models\n",
    "nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68df521c-68c5-40b4-b257-663e4dca7c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector representation of 'word':\n",
      "[-1.08090392e-03  4.56633745e-04  1.01801734e-02  1.80175044e-02\n",
      " -1.85976196e-02 -1.42322658e-02  1.29208742e-02  1.79547630e-02\n",
      " -1.00438409e-02 -7.52060255e-03  1.47521878e-02 -3.06470832e-03\n",
      " -9.04258620e-03  1.31337121e-02 -9.73099563e-03 -3.60068190e-03\n",
      "  5.78776980e-03  2.04549939e-03 -1.65796690e-02 -1.89189203e-02\n",
      "  1.46660432e-02  1.01537155e-02  1.35440873e-02  1.55066280e-03\n",
      "  1.27366045e-02 -6.78327074e-03 -1.88578723e-03  1.15584703e-02\n",
      " -1.50721520e-02 -7.88861141e-03 -1.50315491e-02 -1.85634918e-03\n",
      "  1.90721173e-02 -1.46396738e-02 -4.67665354e-03 -3.87141411e-03\n",
      "  1.61596984e-02 -1.18871182e-02  7.91366474e-05 -9.53264721e-03\n",
      " -1.91787109e-02  1.00003006e-02 -1.74926352e-02 -8.77535623e-03\n",
      " -3.02592234e-05 -5.68456890e-04 -1.53139336e-02  1.91891249e-02\n",
      "  9.97769367e-03  1.84667874e-02]\n",
      "\n",
      "Most similar words to 'word':\n",
      "type: 0.27071094512939453\n",
      "two: 0.2545482814311981\n",
      "nlp: 0.2410869002342224\n",
      "meaning: 0.21097184717655182\n",
      "words: 0.1865202784538269\n",
      "\n",
      "Embedding for 'word':\n",
      "[-1.08090392e-03  4.56633745e-04  1.01801734e-02  1.80175044e-02\n",
      " -1.85976196e-02 -1.42322658e-02  1.29208742e-02  1.79547630e-02\n",
      " -1.00438409e-02 -7.52060255e-03  1.47521878e-02 -3.06470832e-03\n",
      " -9.04258620e-03  1.31337121e-02 -9.73099563e-03 -3.60068190e-03\n",
      "  5.78776980e-03  2.04549939e-03 -1.65796690e-02 -1.89189203e-02\n",
      "  1.46660432e-02  1.01537155e-02  1.35440873e-02  1.55066280e-03\n",
      "  1.27366045e-02 -6.78327074e-03 -1.88578723e-03  1.15584703e-02\n",
      " -1.50721520e-02 -7.88861141e-03 -1.50315491e-02 -1.85634918e-03\n",
      "  1.90721173e-02 -1.46396738e-02 -4.67665354e-03 -3.87141411e-03\n",
      "  1.61596984e-02 -1.18871182e-02  7.91366474e-05 -9.53264721e-03\n",
      " -1.91787109e-02  1.00003006e-02 -1.74926352e-02 -8.77535623e-03\n",
      " -3.02592234e-05 -5.68456890e-04 -1.53139336e-02  1.91891249e-02\n",
      "  9.97769367e-03  1.84667874e-02]\n",
      "\n",
      "Embedding for 'embeddings':\n",
      "[-0.01725564  0.00732368  0.01036363  0.01148885  0.01492934 -0.0123395\n",
      "  0.00220536  0.01208875 -0.00568946 -0.01234363 -0.00083298 -0.01674086\n",
      " -0.01119013  0.01422387  0.00670708  0.01447862  0.01361693  0.01509185\n",
      " -0.00758376 -0.00113613  0.0047263  -0.00901843  0.01678588 -0.01969987\n",
      "  0.01356449  0.00584905 -0.00985803  0.00879683 -0.00349143  0.01342287\n",
      "  0.01993727 -0.00871814 -0.00119691 -0.0113985   0.00770351  0.00556806\n",
      "  0.0137937   0.01220477  0.01908143  0.01854513  0.01580897 -0.01398641\n",
      " -0.01830995 -0.00070888 -0.00618148  0.01579161  0.01185832 -0.00310652\n",
      "  0.00304434  0.00357726]\n",
      "\n",
      "Embedding for 'machine':\n",
      "[ 2.8711346e-03 -5.2982196e-03 -1.4175140e-02 -1.5613081e-02\n",
      " -1.8247189e-02 -1.1872172e-02 -3.7023493e-03 -8.6522102e-03\n",
      " -1.2935537e-02 -7.4257329e-03  8.5795149e-03 -7.4868444e-03\n",
      "  1.6787630e-02  3.0731156e-03 -1.4500690e-02  1.8882371e-02\n",
      "  1.5274948e-02  1.1016522e-02 -1.3715153e-02  1.1653014e-02\n",
      "  8.0355173e-03  1.0386795e-02  8.5092662e-03  3.8783220e-03\n",
      " -6.3323821e-03  1.6719472e-02  1.9247271e-02  7.6043056e-03\n",
      " -5.6819706e-03  9.4021311e-07  2.4415329e-03 -1.6938306e-02\n",
      " -1.6453039e-02 -4.5920623e-04  2.4573139e-03 -1.1507003e-02\n",
      " -9.4523905e-03 -1.4713274e-02  1.6669003e-02  2.3201927e-04\n",
      " -9.0257460e-03  1.1412839e-02  1.8381495e-02 -8.1884190e-03\n",
      "  1.5943917e-02  1.0758154e-02  1.1755673e-02  1.0042994e-03\n",
      "  1.6436601e-02 -1.4038563e-02]\n",
      "\n",
      "Embedding for 'language':\n",
      "[ 0.01953655  0.01633002  0.00255095  0.01018625  0.00281254 -0.01291264\n",
      " -0.00285762  0.01290403 -0.00923074 -0.00799072  0.00985126  0.00542862\n",
      " -0.00368775 -0.00575538  0.01201521 -0.01142965 -0.00646517 -0.01296876\n",
      " -0.0084741  -0.01716424 -0.00893474 -0.01702203  0.00281328 -0.01722612\n",
      " -0.01983496 -0.01639872 -0.0135451   0.01337573  0.00755871  0.00071228\n",
      " -0.00591964 -0.01485747  0.00106472  0.00100643  0.00039152  0.00170004\n",
      "  0.00157327 -0.00014066 -0.01601202 -0.01174022 -0.01676769 -0.00261876\n",
      "  0.00364599  0.01483375 -0.00391682 -0.00465049  0.01898796  0.00015364\n",
      " -0.00480497  0.01720544]\n",
      "\n",
      "Embedding for 'study':\n",
      "[-0.01915552  0.01787609  0.00832099  0.01847555  0.01328667  0.00585968\n",
      "  0.0196019  -0.00884634 -0.01360867  0.00845631  0.00745025 -0.01133324\n",
      "  0.01941554 -0.0071052   0.01910591  0.00168321 -0.01267453 -0.0039347\n",
      " -0.01476068 -0.00596539  0.00208805  0.01896874  0.01871994 -0.01318537\n",
      "  0.00696008  0.00456589 -0.00497889 -0.01845893  0.00204834 -0.01632975\n",
      "  0.01263845 -0.01160632  0.01106734  0.01965957 -0.00032365  0.00905785\n",
      " -0.00361965  0.01471694  0.0078807  -0.01802981 -0.00479759  0.00725048\n",
      " -0.00020259 -0.00239508 -0.00210634 -0.00334016  0.00121289  0.00831901\n",
      " -0.00849787 -0.00766503]\n"
     ]
    }
   ],
   "source": [
    "# Example corpus\n",
    "# We use a small set of sentences (corpus) to train the Word2Vec model.\n",
    "corpus = [\n",
    "    \"Natural language processing with word embeddings is a fascinating area of study.\",\n",
    "    \"Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation.\",\n",
    "    \"Word2Vec and GloVe are two popular algorithms for generating word embeddings.\",\n",
    "    \"Understanding word embeddings is crucial for improving machine learning models on text data.\",\n",
    "    \"Machine learning and NLP are deeply connected in building intelligent systems.\"\n",
    "]\n",
    "\n",
    "# Tokenize the sentences into words\n",
    "# We tokenize each sentence into words using NLTK's word_tokenize. The sentences are converted to lowercase to maintain uniformity.\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "\n",
    "\n",
    "# Train the Word2Vec model on the tokenized sentences\n",
    "# vector_size=50: The dimensionality of the word vectors.\n",
    "# window=3: The maximum distance between the current and predicted word within a sentence.\n",
    "# min_count=1: Ignores all words with a total frequency lower than this value.\n",
    "# workers=4: The number of threads to use while training.\n",
    "model = Word2Vec(tokenized_sentences, vector_size=50, window=3, min_count=1, workers=4)\n",
    "\n",
    "# Check the vector representation of the word 'word'\n",
    "# We can access the learned vector representation of any word in the vocabulary using model.wv['word'].\n",
    "# This gives us a 50-dimensional vector representing the word in the vector space.\n",
    "\n",
    "word_vector = model.wv['word']\n",
    "print(\"Vector representation of 'word':\")\n",
    "print(word_vector)\n",
    "\n",
    "# Find words most similar to 'word'\n",
    "# Using the model.wv.most_similar() function, we can find words that are most similar to a given word based on the cosine similarity of their vector representations. For example, we find the top 5 words most similar to \"word\".\n",
    "similar_words = model.wv.most_similar('word', topn=5)\n",
    "print(\"\\nMost similar words to 'word':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity}\")\n",
    "\n",
    "# Visualize the learned word vectors for a few words\n",
    "words_to_visualize = ['word', 'embeddings', 'machine', 'language', 'study']\n",
    "\n",
    "for word in words_to_visualize:\n",
    "    print(f\"\\nEmbedding for '{word}':\")\n",
    "    print(model.wv[word])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afbbcab9-9b8d-4c22-a265-929e298dc4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model can be saved and loaded for later use\n",
    "model.save(\"../model/word2vec.model\")\n",
    "loaded_model = gensim.models.Word2Vec.load(\"../model/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e4f29-0fb3-489f-885b-9513dca69180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
