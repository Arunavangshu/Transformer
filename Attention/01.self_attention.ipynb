{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d207f71f-6af5-481a-9c00-fe89ccb5887d",
   "metadata": {},
   "source": [
    "# Understanding Self-Attention: The Core Mechanism Behind Transformers\n",
    "\n",
    "In the world of deep learning, particularly in Natural Language Processing (NLP) and computer vision, **self-attention** has become one of the most important mechanisms that drive state-of-the-art models. Self-attention is at the heart of the **Transformer architecture**, which has revolutionized the way machines understand and generate text. In this blog post, we will dive deep into the concept of self-attention, its workings, and why it's such a pivotal innovation in modern AI.\n",
    "\n",
    "\n",
    "### 1. What is Self-Attention?\n",
    "At its core, **self-attention** is a mechanism that allows a model to weigh the importance of different words in a sentence (or elements in a sequence) with respect to each other. Unlike traditional models that process words sequentially (e.g., RNNs and LSTMs), self-attention enables each word to attend to every other word in the sequence simultaneously, learning complex relationships between them.\n",
    "\n",
    "In simpler terms, self-attention helps a model decide which parts of the input sequence are most relevant when producing the output for a particular word. This makes it highly effective for handling long-range dependencies in data, such as long sentences in NLP or distant pixels in images.\n",
    "\n",
    "\n",
    "### 2. Why Do We Need Self-Attention?\n",
    "Before self-attention, sequence data was traditionally processed by models like Recurrent Neural Networks (RNNs) or Long Short-Term Memory networks (LSTMs). While these models could process sequences, they had a major limitation: **sequential processing**. RNNs and LSTMs process data one step at a time, making them inherently slow and prone to problems with long-range dependencies (vanishing gradients).\n",
    "\n",
    "**Self-attention** overcomes these limitations by processing the entire sequence in parallel, allowing the model to capture dependencies between words (or other elements) regardless of their distance in the sequence. This parallelization leads to significant speed-ups during training and inference, while also providing the model with a better understanding of the context between words.\n",
    "\n",
    "\n",
    "### 3. How Does Self-Attention Work?\n",
    "Self-attention works by computing a set of **attention scores** for each element in the sequence, indicating how much focus each word should give to every other word. This process involves three key components:\n",
    "\n",
    "\n",
    "#### Query, Key, and Value Vectors\n",
    "\n",
    "Each word in the input sequence is transformed into three vectors:\n",
    "\n",
    "+ **Query (Q):** Represents the word you are currently focusing on.\n",
    "+ **Key (K):** Represents the potential words that could be attended to.\n",
    "+ **Value (V):** Represents the actual information that will be passed on once the attention scores are calculated.\n",
    "\n",
    "These vectors are obtained by multiplying the input embeddings (representations of words) with learned weight matrices. The **Query** determines what information to look for, the **Key** tells the model where the relevant information is located, and the **Value** holds the information that gets passed through after attention scores are applied.\n",
    "\n",
    "\n",
    "+ $ \\mathbf{Q = Embedding * W_Q} $\n",
    "+ $ \\mathbf{K = Embedding * W_K} $\n",
    "+ $ \\mathbf{V = Embedding * W_V} $\n",
    "\n",
    "The weights $ \\mathbf{W_Q} $, $ \\mathbf{W_K} $, and $ \\mathbf{W_V} $ are learned during training.\n",
    "\n",
    "\n",
    "\n",
    "#### Attention Scores\n",
    "\n",
    "The next step is to compute the **attention scores**, which quantify the relevance between the current word (query) and all other words (keys). This is done by calculating the dot product between the query vector and key vector for every pair of words in the sequence.\n",
    "\n",
    "The attention score $ \\alpha_{ij} $ between the query $ Q_i $ and key $ K_j $ is given by $$ \\alpha_{ij} = \\frac{Q_i \\cdot K_j}{\\sqrt{d_k}} $$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $ Q_i $ and $ K_j $ are the query and key vectors for words $ i $ and $ j $, respectively.\n",
    "- $ d_k $ is the dimension of the key vectors (used for scaling to avoid large values).\n",
    "\n",
    "\n",
    "#### Softmax and Normalization\n",
    "\n",
    "Once the attention scores are computed, they are passed through a softmax function to normalize the scores. This makes the scores sum to 1 and turns them into probabilities, which are easier to interpret as a weighted attention distribution:\n",
    "$$ \\text{Attention Weight}_{ij} = \\text{Softmax}(\\alpha_{ij}) = \\frac{e^{\\alpha_{ij}}}{\\sum_k e^{\\alpha_{ik}}} $$\n",
    "\n",
    "\n",
    "Now, the attention weights determine how much each word should focus on others in the sequence. Higher attention weights mean that the word is more important for computing the current word's output.\n",
    "\n",
    "\n",
    "#### Weighted Sum of Values\n",
    "\n",
    "Finally, the attention weights are used to compute a weighted sum of the value vectors for each word. This produces an output representation that is a mixture of information from all the words in the sequence, weighted by their attention scores.\n",
    "\n",
    "$$\n",
    "Output_i = \\sum_j \\text{Attention Weight}_{ij} \\times V_j\n",
    "$$\n",
    "\n",
    "\n",
    "This output is a new representation of word $i$ enriched with context from other words in the sequence.\n",
    "\n",
    "#### Why Scale the Dot Product? $ (\\frac{Q_i \\cdot K_j}{\\mathbf{\\sqrt{d_k}}})$\n",
    "\n",
    "When the **dimension of the query and key vectors** is large, the dot product can become **large in magnitude**. This is due to the fact that as the dimensionality $d_k$ increases, the values in the query and key vectors are more likely to have higher values, which makes the dot product larger.\n",
    "\n",
    "Now, when we apply the **softmax function** to the attention scores, we want to avoid extremely large or small values, because softmax has the tendency to push very large values to 1 and very small values to 0. This would lead to a situation where the model pays attention to only a single word (the one with the largest attention score), which is not ideal, especially when we're dealing with complex relationships in a sequence.\n",
    "\n",
    "To control the scale of the attention scores and maintain a balanced distribution, we divide the dot product by the square root of the dimension of the key vector $d_k$. This ensures that the dot products stay within a reasonable range, making the softmax function behave more smoothly. This scaling factor (${\\sqrt{d_k}}$) effectively normalizes the dot product and prevents the scores from becoming too large as the dimensionality of the vectors increases. Thus we maintain numerical stability and avoid the model focusing too much on a single word or ignoring important words because of disproportionately large values.\n",
    "\n",
    "### 4. Multi-Head Attention\n",
    "A major enhancement of the self-attention mechanism is **multi-head attention**. Instead of computing a single attention score for each pair of words, multi-head attention computes several attention scores in parallel, using different learned projections of the query, key, and value vectors.\n",
    "\n",
    "Each attention head captures a different aspect of the relationships between words. By concatenating the outputs from all attention heads and passing them through a final linear layer, the model can capture richer and more diverse relationships in the data.\n",
    "\n",
    "This parallelization allows the model to attend to different parts of the sequence from different perspectives, leading to better performance.\n",
    "\n",
    "### 5. Benefits of Self-Attention\n",
    "Self-attention offers several key benefits:\n",
    "\n",
    "+ **Parallelization:** Unlike RNNs, which process sequences step-by-step, self-attention can process entire sequences in parallel, speeding up both training and inference.\n",
    "+ **Long-Range Dependencies:** Self-attention can capture long-range dependencies in data, unlike RNNs or LSTMs, which struggle with information that is far apart in the sequence.\n",
    "+ **Scalability:** The self-attention mechanism scales well with the length of the sequence, as each word attends to all other words in constant time, leading to efficient computations.\n",
    "+ **Flexibility:** Self-attention works equally well in NLP, computer vision, and other fields where relationships between elements need to be modeled, making it highly versatile.\n",
    "\n",
    "### 6. Applications of Self-Attention\n",
    "Self-attention is the backbone of several groundbreaking models that have achieved state-of-the-art performance across a variety of tasks:\n",
    "\n",
    "+ **Machine Translation:** The Transformer, with self-attention, is used for high-quality machine translation, such as in Google Translate.\n",
    "+ **Text Generation:** Models like GPT use self-attention to generate coherent, contextually relevant text.\n",
    "+ **Text Classification:** BERT-based models have been fine-tuned for tasks like sentiment analysis, question answering, and document classification.\n",
    "+ **Vision:** Vision Transformers (ViT) apply self-attention to image patches, enabling the model to learn global dependencies between pixels.\n",
    "+ **Speech Recognition:** Self-attention models are also used in speech-to-text tasks, improving transcription accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1820300f-4242-4c9b-b9f8-4fa5cf9bf635",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a4b8b8c-7957-48a2-8160-4d9329c5833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6fe5682-562b-4092-9ff4-53e77e7b8b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, d_model, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size  # Dimensionality of input embeddings\n",
    "        self.d_model = d_model  # Dimensionality of the model (hidden layers)\n",
    "        self.heads = heads\n",
    "        self.head_dim = d_model // heads  # Head dimension\n",
    "\n",
    "        # Ensure the d_model is divisible by the number of heads\n",
    "        assert self.head_dim * heads == d_model, \"d_model must be divisible by number of heads\"\n",
    "\n",
    "        # Linear layers for queries, keys, and values (projecting from embed_size to d_model)\n",
    "        self.values = nn.Linear(embed_size, d_model)  # Embedding to model size (W_V)\n",
    "        self.keys = nn.Linear(embed_size, d_model)    # Embedding to model size (W_K)\n",
    "        self.queries = nn.Linear(embed_size, d_model) # Embedding to model size (W_Q)\n",
    "\n",
    "        # Output linear layer to combine multi-head output\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size = x.shape[0]  # Batch size\n",
    "        seq_len = x.shape[1]     # Sequence length\n",
    "\n",
    "        # Step 1: Pass the input word embeddings (x) through linear layers\n",
    "        queries = self.queries(x)  # (batch_size, seq_len, d_model)\n",
    "        keys = self.keys(x)        # (batch_size, seq_len, d_model)\n",
    "        values = self.values(x)    # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Step 2: Split the embeddings into multiple heads\n",
    "        queries = queries.reshape(batch_size, seq_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(batch_size, seq_len, self.heads, self.head_dim)\n",
    "        values = values.reshape(batch_size, seq_len, self.heads, self.head_dim)\n",
    "\n",
    "        # Transpose to get the shape (batch_size, heads, seq_len, head_dim)\n",
    "        queries = queries.permute(0, 2, 1, 3)\n",
    "        keys = keys.permute(0, 2, 1, 3)\n",
    "        values = values.permute(0, 2, 1, 3)\n",
    "\n",
    "        # Step 3: Calculate attention scores (Q * K^T)\n",
    "        energy = torch.matmul(queries, keys.permute(0, 1, 3, 2))  # Shape: (batch_size, heads, seq_len, seq_len)\n",
    "\n",
    "        # Step 4: Apply mask (if any)\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))  # Apply mask\n",
    "\n",
    "        # Step 5: Normalize the energy scores to get attention weights\n",
    "        attention = torch.softmax(energy / (self.head_dim ** (1 / 2)), dim=-1)  # Shape: (batch_size, heads, seq_len, seq_len)\n",
    "\n",
    "        # Step 6: Compute the weighted sum of values\n",
    "        out = torch.matmul(attention, values)  # (batch_size, heads, seq_len, head_dim)\n",
    "\n",
    "        # Step 7: Reshape the output back to (batch_size, seq_len, d_model)\n",
    "        out = out.permute(0, 2, 1, 3).contiguous().reshape(batch_size, seq_len, self.heads * self.head_dim)\n",
    "\n",
    "        # Step 8: Apply the final linear layer\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c7fcf48-3581-4759-a33c-a206bdfa59a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([4, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "# Test the SelfAttention module\n",
    "if __name__ == \"__main__\":\n",
    "    embed_size = 128  # Dimensionality of input embeddings (e.g., word embeddings)\n",
    "    d_model = 256     # Dimensionality of the model (hidden layers)\n",
    "    heads = 8         # Number of attention heads\n",
    "    seq_length = 10   # Length of the sequence (e.g., sentence length)\n",
    "    batch_size = 4    # Number of samples in the batch\n",
    "\n",
    "    # Random input tensor representing a batch of word embeddings\n",
    "    x = torch.rand((batch_size, seq_length, embed_size))  # (batch_size, seq_len, embed_size)\n",
    "\n",
    "    # Instantiate the SelfAttention module\n",
    "    self_attention = SelfAttention(embed_size, d_model, heads)\n",
    "\n",
    "    # Perform forward pass\n",
    "    out = self_attention(x)\n",
    "\n",
    "    print(\"Output shape:\", out.shape)  # Expected shape: (batch_size, seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09db5498-c3ad-4ffd-a168-97412c78f750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
